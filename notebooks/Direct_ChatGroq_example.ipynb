{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a181f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-30 22:27:31,613 - src.config - INFO - config.py:49 - <module> - Environment configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# Append the current and parent directories to the system path\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.config import settings, logger\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = settings.GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d57f1",
   "metadata": {},
   "source": [
    "### Create LLM using Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c2632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.MODEL_NAME = \"openai/gpt-oss-20b\" # support reasoning\n",
    "llm = ChatGroq(\n",
    "    model=settings.MODEL_NAME,\n",
    "    temperature=settings.TEMPERATURE,\n",
    "    max_tokens=settings.MAX_TOKENS,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=settings.TIMEOUT,\n",
    "    max_retries=settings.MAX_RETERIES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8ba328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**French translation:**  \n",
      "> Jâ€™aime la programmation et Pandas.\n",
      "\n",
      "**Wordâ€‘byâ€‘word breakdown**\n",
      "\n",
      "| English | French |\n",
      "|---------|--------|\n",
      "| I | Je |\n",
      "| love | aime |\n",
      "| programming | la programmation |\n",
      "| and | et |\n",
      "| Pandas | Pandas |\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence. \"\n",
    "        \"Then give me word to word translations\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming and Pandas.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda334d5",
   "metadata": {},
   "source": [
    "### Multimodel Analysis\n",
    "\n",
    "As of 11.2025, Langchain ChatGroq does not support multimodal inputs. Thus, Groq library is used directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2be3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image features a collection of stickers showcasing llamas, cacti, and hearts on a pink background with white polka dots. The stickers are arranged in two rows, with the top row containing the text \"Llama Love You\" in black cursive font.\\n\\n**Sticker Details:**\\n\\n*   **Llama Stickers:** There are five llama stickers, each with a unique design. They have light-brown fur and are adorned with colorful blankets featuring tassels and hearts.\\n*   **Cactus Stickers:** Three cactus stickers are present, each in a different pot. The pots are colored blue, orange, and pink, respectively.\\n*   **Heart Stickers:** Two heart stickers are visible, one large pink heart and one smaller pink heart with a white outline.\\n\\n**Background:**\\n\\n*   The background of the image is a soft pink color with white polka dots scattered throughout.\\n\\n**Overall Impression:**\\n\\n*   The image appears to be a digital illustration or graphic design, likely created for use as stickers or other craft supplies. The overall aesthetic is playful and whimsical, with a focus on cute and endearing designs.', role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return encoded_string\n",
    "\n",
    "image_path = r\"..\\..\\Agentic_Workflow\\notebooks\\lama.jpg\"\n",
    "base64_image = encode_image_to_base64(image_path)\n",
    "\n",
    "client = Groq(api_key=settings.GROQ_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me more about the area.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=settings.TEMPERATURE,\n",
    "    max_completion_tokens=settings.MAX_TOKENS,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2eaf05",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dddd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather_forcast(loc: str):\n",
    "    \"\"\"\n",
    "    Get the weather forcast of the given day\n",
    "    :param loc: string loc. Send the input as {'loc': 'loc_value'}\n",
    "    :return: the weather forcast\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://wttr.in/{loc}?format=j1\"\n",
    "        response = requests.get(url, timeout=20)\n",
    "        weather_data = response.json()\n",
    "        area = weather_data[\"nearest_area\"][0]\n",
    "        area_name = area[\"areaName\"][0][\"value\"]\n",
    "        country = area[\"country\"][0][\"value\"]\n",
    "        region = area.get(\"region\", [{\"value\": \"\"}])[0][\"value\"]\n",
    "\n",
    "        location_str = f\"{area_name}\"\n",
    "        if region and region != area_name:\n",
    "            location_str += f\", {region}\"\n",
    "        location_str += f\", {country}\"\n",
    "\n",
    "        forecast_report = f\"ğŸ“… **{3}-Day Weather Forecast for {location_str}**\\n\\n\"\n",
    "\n",
    "        for i, day in enumerate(weather_data[\"weather\"][:3]):\n",
    "            date = day[\"date\"]\n",
    "            max_temp_c = day[\"maxtempC\"]\n",
    "            max_temp_f = day[\"maxtempF\"]\n",
    "            min_temp_c = day[\"mintempC\"]\n",
    "            min_temp_f = day[\"mintempF\"]\n",
    "\n",
    "            # Get the most representative weather description (usually around midday)\n",
    "            hourly = day[\"hourly\"]\n",
    "            midday_weather = hourly[len(hourly)//2] if hourly else hourly[0]\n",
    "            weather_desc = midday_weather[\"weatherDesc\"][0][\"value\"]\n",
    "\n",
    "            day_label = \"Today\" if i == 0 else (\"Tomorrow\" if i == 1 else date)\n",
    "\n",
    "            forecast_report += f\"**{day_label} ({date}):**\\n\"\n",
    "            forecast_report += f\"ğŸŒ¡ï¸ High: {max_temp_c}Â°C ({max_temp_f}Â°F) | Low: {min_temp_c}Â°C ({min_temp_f}Â°F)\\n\"\n",
    "            forecast_report += f\"â˜ï¸ {weather_desc}\\n\\n\"\n",
    "\n",
    "        # print(forecast_report)\n",
    "        return forecast_report\n",
    "    except Exception as e:\n",
    "        return (f\"Error happened during get_weather_forcast call: {e}\"\n",
    "                f\"Check the error and if possible, try again.\")\n",
    "        \n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1826b11",
   "metadata": {},
   "source": [
    "### Create react agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6601dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 23:09:34,218 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-24 23:09:34,421 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"what's 42 x 7?\", additional_kwargs={}, response_metadata={}, id='bbcd78a1-13e8-4b88-9d52-a631daaec984'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the multiply function.', 'tool_calls': [{'id': 'fc_0e3f4edb-6110-479d-adc8-9e3a615beeb1', 'function': {'arguments': '{\"a\":42,\"b\":7}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 183, 'total_tokens': 218, 'completion_time': 0.038652656, 'prompt_time': 0.020612352, 'queue_time': 0.319166156, 'total_time': 0.059265008}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--98d6b84e-50d5-4f26-bd64-e3400a012fe8-0', tool_calls=[{'name': 'multiply', 'args': {'a': 42, 'b': 7}, 'id': 'fc_0e3f4edb-6110-479d-adc8-9e3a615beeb1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 183, 'output_tokens': 35, 'total_tokens': 218}),\n",
       "  ToolMessage(content='294', name='multiply', id='02f9fb3a-023c-45cd-a3c9-a3e358fe9184', tool_call_id='fc_0e3f4edb-6110-479d-adc8-9e3a615beeb1'),\n",
       "  AIMessage(content='42 Ã— 7 = **294**', additional_kwargs={'reasoning_content': 'The user asked \"what\\'s 42 x 7?\" We answered 294. That is correct. No further action needed.'}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 214, 'total_tokens': 258, 'completion_time': 0.043024635, 'prompt_time': 0.014526632, 'queue_time': 0.085530797, 'total_time': 0.057551267}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a080bd26-d5d7-4c77-a9a1-202e0b58c603-0', usage_metadata={'input_tokens': 214, 'output_tokens': 44, 'total_tokens': 258})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[multiply, get_weather_forcast]\n",
    ")\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0322d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agent**: 42 Ã— 7 = **294**\n"
     ]
    }
   ],
   "source": [
    "print(f\"**Agent**: {response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62fb3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 23:09:53,027 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-24 23:09:53,828 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='How is the weather in Berlin?', additional_kwargs={}, response_metadata={}, id='6e87993a-5c69-4a58-a559-7648d194d0bf'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to call get_weather_forcast with loc=\"Berlin\".', 'tool_calls': [{'id': 'fc_65344c96-f5cc-488a-838e-05e00b32aa44', 'function': {'arguments': '{\"loc\":\"Berlin\"}', 'name': 'get_weather_forcast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 182, 'total_tokens': 221, 'completion_time': 0.038282409, 'prompt_time': 0.010247122, 'queue_time': 0.088031004, 'total_time': 0.048529531}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--0d2f656b-8db9-4304-bf9d-279c7e9acdc4-0', tool_calls=[{'name': 'get_weather_forcast', 'args': {'loc': 'Berlin'}, 'id': 'fc_65344c96-f5cc-488a-838e-05e00b32aa44', 'type': 'tool_call'}], usage_metadata={'input_tokens': 182, 'output_tokens': 39, 'total_tokens': 221}), ToolMessage(content='ğŸ“… **3-Day Weather Forecast for Berlin, Germany**\\n\\n**Today (2025-09-24):**\\nğŸŒ¡ï¸ High: 17Â°C (63Â°F) | Low: 8Â°C (47Â°F)\\nâ˜ï¸ Sunny\\n\\n**Tomorrow (2025-09-25):**\\nğŸŒ¡ï¸ High: 17Â°C (62Â°F) | Low: 9Â°C (48Â°F)\\nâ˜ï¸ Partly Cloudy \\n\\n**2025-09-26 (2025-09-26):**\\nğŸŒ¡ï¸ High: 17Â°C (63Â°F) | Low: 10Â°C (51Â°F)\\nâ˜ï¸ Partly Cloudy \\n\\n', name='get_weather_forcast', id='89a9de5b-0804-48a0-a86e-d30bf44faa6e', tool_call_id='fc_65344c96-f5cc-488a-838e-05e00b32aa44'), AIMessage(content='**Berlin Weather (Today â€“ 24\\u202fSep\\u202f2025)**  \\n- **High:** 17\\u202fÂ°C (63\\u202fÂ°F)  \\n- **Low:** 8\\u202fÂ°C (47\\u202fÂ°F)  \\n- **Conditions:** Sunny  \\n\\n**Tomorrow â€“ 25\\u202fSep\\u202f2025**  \\n- **High:** 17\\u202fÂ°C (62\\u202fÂ°F)  \\n- **Low:** 9\\u202fÂ°C (48\\u202fÂ°F)  \\n- **Conditions:** Partly cloudy  \\n\\n**Day after â€“ 26\\u202fSep\\u202f2025**  \\n- **High:** 17\\u202fÂ°C (63\\u202fÂ°F)  \\n- **Low:** 10\\u202fÂ°C (51\\u202fÂ°F)  \\n- **Conditions:** Partly cloudy  \\n\\nFeel free to let me know if youâ€™d like more details (e.g., wind, humidity, precipitation chances).', additional_kwargs={'reasoning_content': 'The user asked: \"How is the weather in Berlin?\" We responded with a forecast. That seems fine. The user might want more details or a different format. But the answer is correct. We can provide a concise answer.'}, response_metadata={'token_usage': {'completion_tokens': 225, 'prompt_tokens': 348, 'total_tokens': 573, 'completion_time': 0.253071297, 'prompt_time': 0.029204019, 'queue_time': 0.109606905, 'total_time': 0.282275316}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--890d32df-b6f3-40b2-9ac5-7fe07c3fe461-0', usage_metadata={'input_tokens': 348, 'output_tokens': 225, 'total_tokens': 573})]}\n",
      "**Agent**: **Berlin Weather (Today â€“ 24â€¯Sepâ€¯2025)**  \n",
      "- **High:** 17â€¯Â°C (63â€¯Â°F)  \n",
      "- **Low:** 8â€¯Â°C (47â€¯Â°F)  \n",
      "- **Conditions:** Sunny  \n",
      "\n",
      "**Tomorrow â€“ 25â€¯Sepâ€¯2025**  \n",
      "- **High:** 17â€¯Â°C (62â€¯Â°F)  \n",
      "- **Low:** 9â€¯Â°C (48â€¯Â°F)  \n",
      "- **Conditions:** Partly cloudy  \n",
      "\n",
      "**Day after â€“ 26â€¯Sepâ€¯2025**  \n",
      "- **High:** 17â€¯Â°C (63â€¯Â°F)  \n",
      "- **Low:** 10â€¯Â°C (51â€¯Â°F)  \n",
      "- **Conditions:** Partly cloudy  \n",
      "\n",
      "Feel free to let me know if youâ€™d like more details (e.g., wind, humidity, precipitation chances).\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"How is the weather in Berlin?\"}]})\n",
    "print(response)\n",
    "print(f\"**Agent**: {response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c6fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
