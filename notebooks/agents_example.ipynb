{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a181f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "# Append the current and parent directories to the system path\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.config import settings, logger\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = settings.GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d57f1",
   "metadata": {},
   "source": [
    "### Create LLM using Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c2632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=settings.MODEL_NAME,\n",
    "    temperature=settings.TEMPERATURE,\n",
    "    max_tokens=settings.MAX_TOKENS,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=settings.TIMEOUT,\n",
    "    max_retries=settings.MAX_RETERIES,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea8ba328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:38:03,772 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of the sentence \"I love programming and Pandas.\" into French is \"J'aime la programmation et Pandas.\" Here is the word-to-word breakdown:\n",
      "\n",
      "1. I - J'\n",
      "2. love - aime\n",
      "3. programming - la programmation\n",
      "4. and - et\n",
      "5. Pandas - Pandas\n",
      "\n",
      "This breakdown accurately reflects each word's translation, maintaining the proper structure and meaning in French.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence. \"\n",
    "        \"Then give me word to word translations\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming and Pandas.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2eaf05",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dddd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather_forcast(loc: str):\n",
    "    \"\"\"\n",
    "    Get the weather forcast of the given day\n",
    "    :param loc: string loc. Send the input as {'loc': 'loc_value'}\n",
    "    :return: the weather forcast\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://wttr.in/{loc}?format=j1\"\n",
    "        response = requests.get(url, timeout=20)\n",
    "        weather_data = response.json()\n",
    "        area = weather_data[\"nearest_area\"][0]\n",
    "        area_name = area[\"areaName\"][0][\"value\"]\n",
    "        country = area[\"country\"][0][\"value\"]\n",
    "        region = area.get(\"region\", [{\"value\": \"\"}])[0][\"value\"]\n",
    "\n",
    "        location_str = f\"{area_name}\"\n",
    "        if region and region != area_name:\n",
    "            location_str += f\", {region}\"\n",
    "        location_str += f\", {country}\"\n",
    "\n",
    "        forecast_report = f\"📅 **{3}-Day Weather Forecast for {location_str}**\\n\\n\"\n",
    "\n",
    "        for i, day in enumerate(weather_data[\"weather\"][:3]):\n",
    "            date = day[\"date\"]\n",
    "            max_temp_c = day[\"maxtempC\"]\n",
    "            max_temp_f = day[\"maxtempF\"]\n",
    "            min_temp_c = day[\"mintempC\"]\n",
    "            min_temp_f = day[\"mintempF\"]\n",
    "\n",
    "            # Get the most representative weather description (usually around midday)\n",
    "            hourly = day[\"hourly\"]\n",
    "            midday_weather = hourly[len(hourly)//2] if hourly else hourly[0]\n",
    "            weather_desc = midday_weather[\"weatherDesc\"][0][\"value\"]\n",
    "\n",
    "            day_label = \"Today\" if i == 0 else (\"Tomorrow\" if i == 1 else date)\n",
    "\n",
    "            forecast_report += f\"**{day_label} ({date}):**\\n\"\n",
    "            forecast_report += f\"🌡️ High: {max_temp_c}°C ({max_temp_f}°F) | Low: {min_temp_c}°C ({min_temp_f}°F)\\n\"\n",
    "            forecast_report += f\"☁️ {weather_desc}\\n\\n\"\n",
    "\n",
    "        # print(forecast_report)\n",
    "        return forecast_report\n",
    "    except Exception as e:\n",
    "        return (f\"Error happened during get_weather_forcast call: {e}\"\n",
    "                f\"Check the error and if possible, try again.\")\n",
    "        \n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1826b11",
   "metadata": {},
   "source": [
    "### Create react agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6601dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:39:28,546 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 22:39:29,226 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 22:39:30,428 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"what's 42 x 7?\", additional_kwargs={}, response_metadata={}, id='9541f819-2e47-4579-a8ee-7aeeb9bbc32d'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Alright, the user is asking \"what\\'s 42 x 7?\" So, I need to figure out how to respond using the tools provided. \\n\\nFirst, I check the available tools. There\\'s a \"multiply\" function that takes two integers, a and b. That seems perfect for this calculation.\\n\\nI should call the multiply function with a=42 and b=7. I\\'ll structure the JSON accordingly within the tool_call tags.\\n\\nDouble-checking the function\\'s parameters to make sure I\\'m using the correct keys and data types. Everything looks good, so I\\'ll format the response as specified.\\n', 'tool_calls': [{'id': '1w3ed42tg', 'function': {'arguments': '{\"a\":42,\"b\":7}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 201, 'total_tokens': 355, 'completion_time': 0.618855244, 'prompt_time': 0.027340997, 'queue_time': 0.085218569, 'total_time': 0.646196241}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9d8867ce-d812-4644-acaf-7ca4e873dbd4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 42, 'b': 7}, 'id': '1w3ed42tg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 201, 'output_tokens': 154, 'total_tokens': 355}),\n",
       "  ToolMessage(content='294', name='multiply', id='7d6ed177-fc50-4bf2-8478-5a8bf99d0a33', tool_call_id='1w3ed42tg'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"what\\'s 42 x 7?\" So, I need to figure out how to respond using the tools provided.\\n\\nLooking at the tools, there\\'s a multiply function that takes two integers, a and b. That seems perfect for this calculation.\\n\\nI should call the multiply function with a=42 and b=7. So, the arguments would be {\"a\":42, \"b\":7}.\\n\\nI\\'ll format the tool call as specified, using the name \"multiply\" and the arguments I just determined.\\n', 'tool_calls': [{'id': 'p1v4frh67', 'function': {'arguments': '{\"a\":42,\"b\":7}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 137, 'prompt_tokens': 254, 'total_tokens': 391, 'completion_time': 0.499364359, 'prompt_time': 0.03187543, 'queue_time': 0.084239033, 'total_time': 0.531239789}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--df4c3a88-145d-4797-b503-0d7afe3f619b-0', tool_calls=[{'name': 'multiply', 'args': {'a': 42, 'b': 7}, 'id': 'p1v4frh67', 'type': 'tool_call'}], usage_metadata={'input_tokens': 254, 'output_tokens': 137, 'total_tokens': 391}),\n",
       "  ToolMessage(content='294', name='multiply', id='982aa034-bb02-4039-9b76-8dcc64ec9ce0', tool_call_id='p1v4frh67'),\n",
       "  AIMessage(content='The result of multiplying 42 by 7 is **294.', additional_kwargs={'reasoning_content': 'Alright, the user is asking for the product of 42 and 7. I remember that 42 multiplied by 7 is a well-known result from \"The Hitchhiker\\'s Guide to the Galaxy,\" where it\\'s presented as the \"Answer to the Ultimate Question of Life, the Universe, and Everything.\" So, I think the user might be referencing that or just looking for a simple multiplication.\\n\\nI should use the multiply function provided. Let me check the function signature: it takes two integers, a and b. So, I\\'ll set a to 42 and b to 7. I\\'ll make sure to format the tool call correctly with the function name and the arguments in a JSON object.\\n\\nAfter sending the tool call, I\\'ll receive the result. Since I know the answer is 294, I can confirm that the function works correctly. I should present the result clearly to the user, maybe with a bit of context if they\\'re interested in the reference.\\n'}, response_metadata={'token_usage': {'completion_tokens': 214, 'prompt_tokens': 308, 'total_tokens': 522, 'completion_time': 0.990812724, 'prompt_time': 0.03661124, 'queue_time': 0.088528559, 'total_time': 1.027423964}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--778be176-ff07-4d5f-ab7e-d6263fa19a6c-0', usage_metadata={'input_tokens': 308, 'output_tokens': 214, 'total_tokens': 522})]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[multiply, get_weather_forcast]\n",
    ")\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0322d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agent**: The result of multiplying 42 by 7 is **294.\n"
     ]
    }
   ],
   "source": [
    "print(f\"**Agent**: {response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:43:16,880 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 22:43:18,695 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-10 22:43:20,363 - httpx - INFO - _client.py - _send_single_request:1025 - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='How is the weather in Berlin?', additional_kwargs={}, response_metadata={}, id='dd14e7a6-8f81-4074-964c-69f43c2edabe'), AIMessage(content='', additional_kwargs={'reasoning_content': \"Okay, the user is asking about the weather in Berlin. I need to figure out which tool to use. Looking at the tools provided, there's one called get_weather_forcast which takes a location as input. That seems perfect for this query. I should call this function with 'Berlin' as the location parameter. So, I'll structure the tool call with the name as get_weather_forcast and the arguments as a JSON object containing loc: 'Berlin'. That should give the user the weather forecast they're asking for.\\n\", 'tool_calls': [{'id': '7017zze1e', 'function': {'arguments': '{\"loc\":\"Berlin\"}', 'name': 'get_weather_forcast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 136, 'prompt_tokens': 200, 'total_tokens': 336, 'completion_time': 0.537596587, 'prompt_time': 0.028136671, 'queue_time': 0.090338411, 'total_time': 0.565733258}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--a732a15b-7b29-4e06-af2f-f3755279c85e-0', tool_calls=[{'name': 'get_weather_forcast', 'args': {'loc': 'Berlin'}, 'id': '7017zze1e', 'type': 'tool_call'}], usage_metadata={'input_tokens': 200, 'output_tokens': 136, 'total_tokens': 336}), ToolMessage(content='📅 **3-Day Weather Forecast for Berlin, Germany**\\n\\n**Today (2025-08-10):**\\n🌡️ High: 25°C (78°F) | Low: 17°C (62°F)\\n☁️ Sunny\\n\\n**Tomorrow (2025-08-11):**\\n🌡️ High: 26°C (78°F) | Low: 15°C (58°F)\\n☁️ Sunny\\n\\n**2025-08-12 (2025-08-12):**\\n🌡️ High: 29°C (85°F) | Low: 17°C (62°F)\\n☁️ Sunny\\n\\n', name='get_weather_forcast', id='152cb7c3-a033-4569-8922-a3e9d62b1fa8', tool_call_id='7017zze1e'), AIMessage(content='', additional_kwargs={'reasoning_content': \"Alright, the user is asking about the weather in Berlin. I need to figure out how to respond using the tools I have. I see that there's a function called get_weather_forcast which takes a location as an argument. So, I'll call that function with 'Berlin' as the location. Once I get the weather data, I'll format it in a clear way for the user, probably listing the forecast for each day with high and low temperatures and conditions. I should make sure the response is easy to read and includes all the necessary information the user might need.\\n\", 'tool_calls': [{'id': '83s8219w4', 'function': {'arguments': '{\"loc\":\"Berlin\"}', 'name': 'get_weather_forcast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 386, 'total_tokens': 528, 'completion_time': 0.57767698, 'prompt_time': 0.043673919, 'queue_time': 0.090314044, 'total_time': 0.621350899}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--0e5d12bb-9f5a-428b-bdb3-dfba3c99b30e-0', tool_calls=[{'name': 'get_weather_forcast', 'args': {'loc': 'Berlin'}, 'id': '83s8219w4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 386, 'output_tokens': 142, 'total_tokens': 528}), ToolMessage(content='📅 **3-Day Weather Forecast for Berlin, Germany**\\n\\n**Today (2025-08-10):**\\n🌡️ High: 25°C (78°F) | Low: 17°C (62°F)\\n☁️ Sunny\\n\\n**Tomorrow (2025-08-11):**\\n🌡️ High: 26°C (78°F) | Low: 15°C (58°F)\\n☁️ Sunny\\n\\n**2025-08-12 (2025-08-12):**\\n🌡️ High: 29°C (85°F) | Low: 17°C (62°F)\\n☁️ Sunny\\n\\n', name='get_weather_forcast', id='74e63231-4b65-43d2-9d8a-2b11dad77497', tool_call_id='83s8219w4'), AIMessage(content=\"The weather in Berlin is expected to be sunny over the next three days, with temperatures ranging from 15°C to 29°C (58°F to 85°F). Here's the detailed forecast:\\n\\n**Today (2025-08-10):**\\n- High: 25°C (78°F)\\n- Low: 17°C (62°F)\\n- Conditions: Sunny\\n\\n**Tomorrow (2025-08-11):**\\n- High: 26°C (78°F)\\n- Low: 15°C (58°F)\\n- Conditions: Sunny\\n\\n**2025-08-12:**\\n- High: 29°C (85°F)\\n- Low: 17°C (62°F)\\n- Conditions: Sunny\\n\\nEnjoy the sunny weather in Berlin!\", additional_kwargs={'reasoning_content': \"Alright, the user is asking about the weather in Berlin again. I remember they asked the same question before, and I provided a detailed 3-day forecast. Since the weather conditions haven't changed much, I can reuse the same information to save time. I'll present it in the same clear format they received earlier. Hopefully, this helps them plan their activities without any issues.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 234, 'prompt_tokens': 572, 'total_tokens': 806, 'completion_time': 0.752982907, 'prompt_time': 0.060698944, 'queue_time': 0.084255194, 'total_time': 0.813681851}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ad6d5377-33e5-462b-b150-1d95e61c35fa-0', usage_metadata={'input_tokens': 572, 'output_tokens': 234, 'total_tokens': 806})]}\n",
      "**Agent**: The weather in Berlin is expected to be sunny over the next three days, with temperatures ranging from 15°C to 29°C (58°F to 85°F). Here's the detailed forecast:\n",
      "\n",
      "**Today (2025-08-10):**\n",
      "- High: 25°C (78°F)\n",
      "- Low: 17°C (62°F)\n",
      "- Conditions: Sunny\n",
      "\n",
      "**Tomorrow (2025-08-11):**\n",
      "- High: 26°C (78°F)\n",
      "- Low: 15°C (58°F)\n",
      "- Conditions: Sunny\n",
      "\n",
      "**2025-08-12:**\n",
      "- High: 29°C (85°F)\n",
      "- Low: 17°C (62°F)\n",
      "- Conditions: Sunny\n",
      "\n",
      "Enjoy the sunny weather in Berlin!\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"How is the weather in Berlin?\"}]})\n",
    "print(response)\n",
    "print(f\"**Agent**: {response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24810f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
